<html lang="en"> <body> 
 <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research related to the Adaptability of DNNs</heading>
              <p>                <a href="index.html">Back to Home</a>               </p>
            </td>
          </tr>
 </tbody></table>
  
              
<!--    ICML 2019  -->
 <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
         
             <td style="padding:20px;width:25%;vertical-align:middle">
              
                  <img src='images/zskd.png' width="160">
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">              
                <papertitle>Zero-Shot Knowledge Distillation in Deep Neural Networks</papertitle>              
              <br>
              Gaurav Kumar Nayak*,
              <strong>Konda Reddy Mopuri*</strong>,
              Vaisakh Shaj*,
              <a href="http://visual-computing.in/wp-content/uploads/2017/08/anirban-chakraborty.html">Anirban Chakraborty</a>,
              <a href="http://cds.iisc.ac.in/faculty/venky/">R. Venkatesh Babu</a>
              <br>
        <em>ICML</em>, 2019  
              <br>
             <a href="http://proceedings.mlr.press/v97/nayak19a/nayak19a.pdf">PDF</a> / <a href=https://github.com/vcl-iisc/ZSKD">Codes</a>
              <p></p>
              <p>We aim to develop novel data-free methods to train the Student from the Teacher. Without even using any meta-data about the target dataset, we attempt to 
              synthesise the synthetic samples (Data Impressions) from the complex Teacher model and utilise these as surrogates for the original training data samples to 
              transfer its learning to Student via knowledge distillation. Therefore, we dub this procedure "Zero-Shot Knowledge Distillation".</p>
            </td>
          </tr> 

 </tbody></table>

<html lang="en"> <!-- <head>   -->
              <p style="padding:20px;width:100%;vertical-align:middle;text-align:center;font-size:20px;">Research related to the Interpretability aspect of DNNs</p>
              <p style="width:100%;vertical-align:middle;text-align:center;font-size:15px"> <a href="index.html">Back to Home</a>               </p>            
          
   <!-- </head> -->
  
  <body> 
  
<!--    WACV 2021  -->
 <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
         
             <td style="padding:20px;width:25%;vertical-align:middle">
              
                  <img src='images/arbitrary-datasets.JPG' width="160">
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">              
                <papertitle>Effectiveness of Arbitrary Transfer Sets for Data-free Knowledge Distillation</papertitle>              
              <br>
              <strong>Konda Reddy Mopuri*</strong>,
              Gaurav Kumar Nayak*,
              <a href="http://visual-computing.in/wp-content/uploads/2017/08/anirban-chakraborty.html">Anirban Chakraborty</a>
              <br>
        <em>WACV</em>, 2021  
              <br>
             <a href="https://arxiv.org/pdf/2011.09113">PDF</a>
              <p></p>
              <p>Knowledge Distillation is an effective method to transfer the learning across deep neural networks. Typically, the dataset originally used for training the Teacher 
                model is chosen as the "Transfer Set" to conduct the knowledge transfer to the Student. However, this original training data may not always be freely available due 
                to privacy or sensitivity concerns. In such scenarios, existing approaches either iteratively compose a synthetic set representative of the original training dataset, 
                one sample at a time or learn a generative model to compose such a transfer set. However, both these approaches involve complex optimization (GAN training or several 
                backpropagation steps to synthesize one sample) and are often computationally expensive. In this paper, as a simple alternative, we investigate the effectiveness of 
                "arbitrary transfer sets" such as random noise, publicly available synthetic, and natural datasets, all of which are completely unrelated to the original training 
                dataset in terms of their visual or semantic contents. Through extensive experiments on multiple benchmark datasets such as MNIST, FMNIST, CIFAR-10 and CIFAR-100, 
                we discover and validate surprising effectiveness of using arbitrary data to conduct knowledge distillation when this dataset is "target-class balanced". 
                We believe that this important observation can potentially lead to designing baselines for the data-free knowledge distillation task.</p>
            </td>
          </tr> 

 </tbody></table>    
              
<!--    ICML 2019  -->
 <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
         
             <td style="padding:20px;width:25%;vertical-align:middle">
              
                  <img src='images/zskd.png' width="160">
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">              
                <papertitle>Zero-Shot Knowledge Distillation in Deep Neural Networks</papertitle>              
              <br>
              <strong>Konda Reddy Mopuri*</strong>,
              Gaurav Kumar Nayak*,              
              Vaisakh Shaj*,
              <a href="http://visual-computing.in/wp-content/uploads/2017/08/anirban-chakraborty.html">Anirban Chakraborty</a>,
              <a href="http://cds.iisc.ac.in/faculty/venky/">R. Venkatesh Babu</a>
              <br>
        <em>ICML</em>, 2019  
              <br>
             <a href="http://proceedings.mlr.press/v97/nayak19a/nayak19a.pdf">PDF</a> / <a href="https://github.com/vcl-iisc/ZSKD">Codes</a>
              <p></p>
              <p>We aim to develop novel data-free methods to train the Student from the Teacher. Without even using any meta-data about the target dataset, we attempt to 
              synthesise the synthetic samples (Data Impressions) from the complex Teacher model and utilise these as surrogates for the original training data samples to 
              transfer its learning to Student via knowledge distillation. Therefore, we dub this procedure "Zero-Shot Knowledge Distillation".</p>
            </td>
          </tr> 

 </tbody></table>

<!--    SPCOM 2018  -->
 <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
         
             <td style="padding:20px;width:25%;vertical-align:middle">
              
                  <img src='images/ss.png' width="160">
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">              
                <papertitle>Learning Representations with Strong Supervision for Image Search</papertitle>              
              <br>
              <strong>Konda Reddy Mopuri</strong>,
              Vishal B Athreya,
              <a href="http://cds.iisc.ac.in/faculty/venky/">R. Venkatesh Babu</a>
              <br>
        <em>SPCOM</em>, 2018 [<mark>Best Paper</mark>]
              <br>
             <a href="https://ieeexplore.ieee.org/document/8724475">Link</a>
             <p></p>
              <p>Tasks such as scene retrieval suffer from features learned from label-level weak supervision and require stronger supervision to better understand the 
               contents of the image. In this paper, we exploit the features learned from caption generating models to learn novel task specific image representations. 
               In particular, we consider captioning system and the dense region description model and demonstrate that, owing to richer supervision provided during their 
               training, features learned by them better than those of CNNs trained on object recognition.</p> 
            </td>
          </tr> 

 </tbody></table>

<!--    ICVGIP 2016  -->
 <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
         
             <td style="padding:20px;width:25%;vertical-align:middle">
              
                  <img src='images/text.png' width="160">
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">              
                <papertitle>Towards semantic visual representation: augmenting image representation with natural language descriptors</papertitle>              
              <br>
              <strong>Konda Reddy Mopuri</strong>,
              <a href="http://cds.iisc.ac.in/faculty/venky/">R. Venkatesh Babu</a>
              <br>
        <em>ICVGIP</em>, 2016  
              <br>
             <a href="https://dl.acm.org/doi/10.1145/3009977.3010010">Link</a>
              <p></p>
              <p>we attempt to enrich the image representation with the tag encodings that leverage their semantics. Our approach utilizes neural network based natural 
               language descriptors to represent the tag information. By complementing the visual features learned by convnets, our approach results in an efficient 
               multi-modal image representation.</p>
            </td>
          </tr> 

 </tbody></table>


<!--    DeepVision CVPR 2015  -->
 <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
         
             <td style="padding:20px;width:25%;vertical-align:middle">
              
                  <img src='images/oldf.jpg' width="160">
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">              
                <papertitle>Object Level Deep Feature (OLDF) Pooling for Compact Image Representation</papertitle>              
              <br>
              <strong>Konda Reddy Mopuri</strong>,
              <a href="http://cds.iisc.ac.in/faculty/venky/">R. Venkatesh Babu</a>
              <br>
        <em>Deep Vision Workshop, CVPR</em>, 2015  
              <br>
             <a href="https://www.cv-foundation.org/openaccess/content_cvpr_workshops_2015/W03/papers/Mopuri_Object_Level_Deep_2015_CVPR_paper.pdf">PDF</a>
              <p></p>
              <p>we demonstrate the effectiveness of the objectness prior over the deep CNN features of image regions for obtaining an invariant image representation. 
               The proposed approach represents the image as a vector of pooled CNN features describing the underlying objects. This representation provides robustness to 
               spatial layout of the objects in the scene and achieves invariance to general geometric transformations, such as translation, rotation and scaling.</p>
            </td>
          </tr> 

 </tbody></table>

<p style="width:100%;vertical-align:middle;text-align:center;font-size:15px"> <a href="index.html">Back to Home</a>               </p>   
<body>

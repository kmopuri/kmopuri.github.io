<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <title>Machine Intelligence Group, IIT Tirupati</title>
  
  <meta name="author" content="Machine Intelligence Group">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/MIG-logo.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:100%;vertical-align:middle">
              <p style="text-align:center">
                <!-- <name>Machine Intelligence Group</name> -->
                <a href="images/MIG-logo-title.png"><img style="width:100%;max-width:85%" alt="profile photo" src="images/MIG-logo-title.png" class="hoverZoomLink"></a>
              </p>
              <p>We are an enthusiastic research lab at <a href="https://iittp.ac.in/">Indian Institute of Technology, Tirupati</a>, led by Dr. Konda Reddy Mopuri.
                The research interests of our group mainly include Machine Learning, Artifical Neural Networks (Deep Learning), Artificial Intelligence, Computer Vision, Image/Signal Processing, Data Science, Optimization, and other related areas.
              </p>
              
              <p style="text-align:center">
                <a href="mailto:mil.iittp@gmail.com">Email</a> &nbsp/&nbsp
                <a href="teaching.html">Teaching</a> &nbsp/&nbsp
                <!-- <a href="openings.html">Openings</a> -->
                <!-- <a href="https://twitter.com/mkreddy48">Twitter</a> -->
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <!-- <a href="images/MIG-logo-title.png"><img style="width:100%;max-width:85%" alt="profile photo" src="images/MIG-logo-title.png" class="hoverZoomLink"></a> -->
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <ul>
                    <!--  <li>Oct 2020: Our work on Early detection of stuck .</li> -->
                
                    <li>Jan 2021: Our work with <a href="https://bozhaonanjing.wixsite.com/mysite">Bo Zhao</a> and <a href="http://homepages.inf.ed.ac.uk/hbilen/">Hakan Bilen</a> has been accepted at the <mark><a href="https://iclr.cc//">ICLR 2021</a></mark> as an <mark><a href="https://openreview.net/pdf?id=mSAKhLYLSsl">Oral paper</a></mark>. It is the second top rated in the whole conference.</li>
                    <li>Jan 2021: Wondered if one can extract out "data" from a trained DNN? Check-out our <a href="https://arxiv.org/abs/2101.06069">pre-print</a>, we do a lot more with the extracted pseudo data samples.</li>
                    <li>Dec 2020: My Thesis has won the <a href=" http://www.iitj.ac.in/icvgip20-21/2020/awards.php">IUPRAI Best Doctoral Dissertation Award</a> for the year 2018-19.</li>
                    <li>Nov 2020: Our paper has been accepted at the <a href="http://wacv2021.thecvf.com/">WACV 2021</a> Conference (CORE A ranked). A version is available <a href="https://arxiv.org/abs/2011.09113">here</a>.</li>
                    <li>Oct 2020: Our paper has been accepted for publishing in the SPE Journal (Impact Factor: 3.372, Scopus Rank:#4/189).</li>
                    <li>Sep 2020: Delivered an invited talk on Knowledge Distillation in data-free scenarios at the Walmart Global Tech India.</li>    
                    <li>Aug 2020: Joined Dept. of CSE, IIT Tirupati as a visiting faculty member.</li>
                    <li>July 2020: My PhD Thesis won the <a href="https://ece.iisc.ac.in/~spcom/2020/bestdoctoraldissertation.html">SPCOM Best Doctoral Disseratation Award</a> for 2018-19.</li>                    
              </ul>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                Following is an approximate clustering and labeling of our research (click on the label to find relevant research).
              </p>
            </td>
          </tr>
        </tbody></table>
                      
<!-- Research 5: Long-Tailed Training Data -->                      
                      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <td style="padding:20px;width:25%;vertical-align:middle">
             <img src='images/longtail.png' width="160">
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="long-tail.html">
                <papertitle>Long-Tailed Training Data</papertitle>
              </a>
              <br>
              
              <p></p>
              <p>Real-world datasets exhibit skewed distributions, generally with a long-tail. In other words, only a few categories contribute majority of the samples, while
                most of the other classes claim relatively small number of samples. Classifiers trained on such data perform poorly on the minority categories. We aim to 
                contribute effective solutions to alleviate the adverse effects casued by class imbalance in the training data.</p>
            </td>
          </tr>     

        </tbody></table>
  
  <!-- Research 4: Data Engineering for Deep Learning -->                      
                      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <td style="padding:20px;width:25%;vertical-align:middle">
             <img src='images/data-engineering.JPG' width="160">
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data-engineering.html">
                <papertitle>Data Engineering for Deep Learning</papertitle>
              </a>
              <br>
              
              <p></p>
              <p>In the digital era with the help of fast growing semiconductor technology we have created heaps of digital content (Images, videos, text, audio, etc.).
                This surely serves the data hungry deep learning in order to read the complex patterns in the data which would be otherwise difficult. However, it comes
                with the costs such as data redundancy, maintanance and distribution overhead, huge computational and time requirements to perform learning activities on these
                digital piles. We aim to investigate engineering solutions to these data and learning related challenges</p>
            </td>
          </tr>     

        </tbody></table>
<!-- Research 1: Robust Deep Learning -->                           
                      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                      
            <td style="padding:20px;width:25%;vertical-align:middle">
             <img src='robustness.jpg' width="160">
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="robustness.html">
                <papertitle>Robustness</papertitle>
              </a>
              <br>
              
              <p></p>
              <p>Deep CNNs are vulnerable to adversarial samples. There have been multiple approaches formulated to generate the adversarial samples. More importantly,
                adversarial samples can be transferred (generalized) across multiple models. This property allows an attacker to launch an attack without knowing the target 
                modelâ€™s internals, which makes them a dangerous threat for deploying the models in practice. Therefore, the effect of adversarial perturbations warrants
                the need for in depth analysis of this subject.</p>
            </td>
          </tr>     

        </tbody></table>

<!-- Research 2: Adaptable Deep Learning -->                      
                      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr onmouseout="ff_stop()" onmouseover="ff_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
             <img src='adaptability.jpg' width="160">
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="adaptability.html">
                <papertitle>Adaptability</papertitle>
              </a>
              <br>
              
              <p></p>
              <p>Deep Learning has been data and resource intensive. However, real-world may challenge us with various constraints to apply these sophisticated tools.
                Adapting deep learning techniques/models (e.g. knowledge transfer, domain adaptation) across tasks and to challenging environments such as low data
                      and data-free scenarios is of high importance.</p>
            </td>
          </tr>     

        </tbody></table>
                      
<!-- Research 3: Interpretable Deep Learning -->                      
                      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <td style="padding:20px;width:25%;vertical-align:middle">
             <img src='interpretability.JPG' width="160">
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="interpretability.html">
                <papertitle>Interpretability</papertitle>
              </a>
              <br>
              
              <p></p>
              <p>Deep learning Models are complex machine learning systems with hundreds of layers and millions of parameters. Presence of advanced regularizers 
                 such as dropout and batch-normalization make the models less transparent. Because of end-to-end nature of the learning, models suffer from lesser
                 decomposability and hence most of us treat them as black-boxes. In order to make these models more transparent, we devise methods that provide 
                 visual explanations to the labels predicted by the recognition CNNs.</p>
            </td>
          </tr>     

        </tbody></table>

                      

<!--Link to the source of the website-->
                   
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Source taken from <a href="https://jonbarron.info/">here</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>

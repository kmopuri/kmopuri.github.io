<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title> Research related to the Robustness of DNNs</title>
</head>
<!--    AMCV CVPR 2020  -->
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

             <td style="padding:20px;width:25%;vertical-align:middle">
              
                  <img src='beyond-flipping.gif' width="160">
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              
                <papertitle>Adversarial Fooling beyond Flipping the Label</papertitle>
              
              <br>
              <strong>Konda Reddy Mopuri*</strong>,
              <a href="https://vaisakh-shaj.github.io/">Vaisakh Shaj*</a>,
              <a href="http://cds.iisc.ac.in/faculty/venky/">R. Venkatesh Babu</a>
              <br>
        <em>AMLCV, CVPR</em>, 2020  
              <br>
              <a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w47/Mopuri_Adversarial_Fooling_Beyond_Flipping_the_Label_CVPRW_2020_paper.pdf">PDF</a>
              <p></p>
              <p>Analyzes the adversarial attacks beyond fooling rate. Existing metrics only look at the percentage of label flips, but we propose metrics to measure 
              the attack's ability on visual and semantic scales.</p>
            </td>
          </tr> 

 </tbody></table>


<!--    ICVGIP 2018  -->

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <td style="padding:20px;width:25%;vertical-align:middle">
              
                  <img src='batchout.JPG' width="160">
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              
                <papertitle>BatchOut: Batch-level feature augmentation to improve robustness to adversarial examples</papertitle>
              
              <br>
              Akshayvarun Subramanya,
              <strong>Konda Reddy Mopuri</strong>,              
              <a href="http://cds.iisc.ac.in/faculty/venky/">R. Venkatesh Babu</a>
              <br>
        <em>ICVGIP</em>, 2018  
              <br>
              <!-- PDF, GitHub, Project page, etc. go here  -->
              <p></p>
              <p>Performs feature space Augmentation to learn  robust deep NNs. In other words, performs efficient Adversarial Training in feature domain
                      unlike the expensive image space.</p>
            </td>
          </tr> 

 </tbody></table>


<!--    ECCV 2018 (A) -->

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <td style="padding:20px;width:25%;vertical-align:middle">
              
                  <img src='images/gat.JPG' width="160">
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              
                <papertitle>GAT: Gray-box Adversarial Training</papertitle>
              
              <br>
              Vivek B S,
              <strong>Konda Reddy Mopuri</strong>,              
              <a href="http://cds.iisc.ac.in/faculty/venky/">R. Venkatesh Babu</a>
              <br>
        <em>ECCV</em>, 2018  
              <br>
              <a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Vivek_B_S_Gray_box_adversarial_ECCV_2018_paper.pdf">PDF</a>
              <p></p>
              <p>We have demonstrated that the pseudo-robustness of the adversarially trained models is due to the shortcomings of the existing evaluation procedure. 
                 To improve the evaluation, we presented an evaluation procedure via constructing robustness plots and a derived metric (Worst-case Performance) that 
                 can assess the susceptibility of the learned models. Further, harnessing our observations we propose a novel variant of adversarial training, termed 
                 Gray-box Adversarial Training to learn robust models.</p>
            </td>
          </tr> 

 </tbody></table>
<!--    ECCV 2018 (B) -->

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <td style="padding:20px;width:25%;vertical-align:middle">
              
                  <img src='images/class-impressions.JPG' width="160">
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              
                <papertitle>Ask, Acquire, and Attack: Data-free UAP Generation using Class Impressions</papertitle>
              
              <br>
              <strong>Konda Reddy Mopuri</strong>,
              Phani Krishna Uppala*,
              <a href="http://cds.iisc.ac.in/faculty/venky/">R. Venkatesh Babu</a>
              <br>
        <em>ECCV</em>, 2018  
              <br>
              <a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Konda_Reddy_Mopuri_Ask_Acquire_and_ECCV_2018_paper.pdf">PDF</a>
              <p></p>
              <p>First attempt to capture the distribution of UAPs for a given CNN classifier in the absence of training data. We extract proxy data from thr target
                classifier called Class Impressions to craft image agnostic adversarial perturbations. In an Adversarial Machine Learning
                framework, we learn a GAN-inspired generative model to capture the set of UAPs for one or more CNN classifiers. The learned generative model can act as
                an oracle which can seed UAPs to fool not only the known classifiers but also the ones not seen during its training. </p>
            </td>
          </tr> 

 </tbody></table>
<!--    CVPR 2018  -->

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <td style="padding:20px;width:25%;vertical-align:middle">
              
                  <img src='images/nag.png' width="160">
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              
                <papertitle>NAG: Network for Adversary Generation</papertitle>
              
              <br>
              <strong>Konda Reddy Mopuri</strong>,
              Utkarsh Ojha*,
              Utsav Garg,
              <a href="http://cds.iisc.ac.in/faculty/venky/">R. Venkatesh Babu</a>
              <br>
        <em>ECCV</em>, 2018  
              <br>
              <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Mopuri_NAG_Network_for_CVPR_2018_paper.pdf">PDF</a>
              <p></p>
              <p>First attempt to capture the distribution of UAPs for a given CNN classifier. In an Adversarial Machine Learning framework, we learn a GAN-inspired
                generative model to capture the set of UAPs for one or more target CNN classifiers. The learned generative model can act as an oracle which can seed 
                UAPs to fool not only the known classifiers but also the ones not seen during its training. </p>
            </td>
          </tr> 

 </tbody></table>
<!--    PAMI 2018  -->

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <td style="padding:20px;width:25%;vertical-align:middle">
              
                  <img src='images/gd-uap.jpg' width="160">
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              
                <papertitle>GD-UAP : Generalizable and data-free objective across vision tasks to craft UAPs</papertitle>
              
              <br>
              <strong>Konda Reddy Mopuri</strong>,
              Aditya Ganeshan*,
              <a href="http://cds.iisc.ac.in/faculty/venky/">R. Venkatesh Babu</a>
              <br>
        <em>Trans. on PAMI</em>, 2018  
              <br>
              <a href="https://arxiv.org/abs/1801.08092">arXiv</a> / <a href="https://github.com/val-iisc/GD-UAP">Codes</a>
              <p></p>
              <p>Generalized Data-Free objective for crafting image agnostic adversarial perturbations. Independent of the underlying task, our objective achieves fooling via
                corrupting the extracted features at multiple layers. Therefore, our objective is generalizable to craft image-agnostic perturbations across vision tasks such 
                as object recognition, semantic segmentation and depth estimation. In black-box attacking scenario our objective outperforms the data dependent objectives. 
                Further, via exploiting simple priors related to the data distribution, our objective remarkably boosts the fooling ability of the crafted perturbations. </p>
            </td>
          </tr> 

 </tbody></table>
<!--    BMVC 2017  -->

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <td style="padding:20px;width:25%;vertical-align:middle">
              
                  <img src='images/fff.jpg' width="160">
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              
                <papertitle>Fast Feature Fool: A data independent approach to universal adversarial perturbations
</papertitle>
              
              <br>
              <strong>Konda Reddy Mopuri*</strong>,
              Utsav Garg*,
              <a href="http://cds.iisc.ac.in/faculty/venky/">R. Venkatesh Babu</a>
              <br>
        <em>BMVC</em>, 2017  
              <br>
              <a href="https://arxiv.org/abs/1707.05572">arXiv</a> / <a href="https://github.com/val-iisc/fast-feature-fool">Codes</a>
              <p></p>
              <p>For the first time, we propose a novel data-free approach to generate image agnostic perturbations for CNNs trained for object recognition.
                These perturbations are transferable across multiple network architectures trained either on same or different data. In the absence of data, our method 
                generates universal perturbations efficiently via fooling the features learned at multiple layers thereby causing CNNs to misclassify. </p>
            </td>
          </tr> 

 </tbody></table>
